{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resume_Screening_Python_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPI6hLISSDABFoQL18QCmCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdshah29/Resume_Screening_Parser/blob/main/Resume_Screening_Python_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:**\n",
        "Resume screening is still the most time-consuming part of recruiting. When a job opening receives 250 resumes on average 70% of them are unqualified, therefore we need a tool that Screens the most appropriate resumes from that long list on the basis of job description. That Ultimately saves a lot of human efforts and essential hours.\n",
        "\n",
        "**Approach Taken:**\n",
        "Please find below steps followed for resume screening parser.\n",
        "-\tAs a first step kept all my data in google drive to access it from COLAB.\n",
        "-\tImport required modules like pdfminer.six, nltk, numpy, pandas subprocess, re etc \n",
        "-\tDeveloped functions to get name, phone number and email address\n",
        "o\tRead resume pdf using pdfminer python module and converted it into text\n",
        "o\tUsed nltk to extract name \n",
        "o\tDeveloped regex to extract email and phone number.\n",
        "-\t Developed functions to get Technical and Non-Technical skills from text \n",
        "o\tread csv files for technical and Non-technical skill to create list.\n",
        "o\tRemoved the stop words from text and apply filter on Technical and Non-technical DB list to get vice-versa skills\n",
        "\n",
        "**Interpretation of Results:**\n",
        "-\tGot exact Email & Phone Number\n",
        "-\tExtracted Name partially \n",
        "-\tAble to get Technical skills  & Non-technical fields \n",
        "\n",
        "**Storyboarding:**\n",
        "\n",
        "Based on the results candidate have below technical skills which is required for the JOB role \n",
        "'Php', 'Java', 'MySQL', 'Github', 'CSS', 'newspaper', 'Framework', 'framework', 'C', 'github', 'Visual', 'PostgreSQL'\n",
        "Also we can tell more on like, we should contact candidate or not if JOB profile and role is provided. \n",
        "\n",
        "**Scope Of Improvement:**\n",
        "\n",
        "Can use NER rather than NLTK to get Name properly, Also technical skills filter parser may improve a bit.\n"
      ],
      "metadata": {
        "id": "ldd4bwqyeHtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAYAI1tbGaPP",
        "outputId": "1f7a4fa8-715a-4ded-c082-3a0a99981869"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20211012)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (36.0.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import python modules and main pdf reader module pdfminer "
      ],
      "metadata": {
        "id": "20DaKdXTHfZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from pdfminer.high_level import extract_text\n",
        "import re\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7fnc3V_Esld",
        "outputId": "d439d16e-c9d4-4caa-8fd7-02eca3f148ee"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to google drive to access resumes "
      ],
      "metadata": {
        "id": "D9FgEwCQHenW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIqd9eIuBcp0",
        "outputId": "93fee4f8-736e-4657-a26b-4ec5a6ca8686"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_path = '/content/gdrive/MyDrive/Resume_Screening'"
      ],
      "metadata": {
        "id": "tVUWQKdJCebP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Function to read txt from PDF "
      ],
      "metadata": {
        "id": "zaJ8sKUTHcxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    return extract_text(pdf_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq2TNskHIVg3",
        "outputId": "3641add5-20b3-441b-af81-7d9adbd2181b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to read names from the text extracted from PDF documents "
      ],
      "metadata": {
        "id": "wMCu4kIoIZVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_names(txt):\n",
        "    person_names = []\n",
        "\n",
        "    for sent in nltk.sent_tokenize(txt):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
        "                person_names.append(\n",
        "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
        "                )\n",
        "\n",
        "    return person_names"
      ],
      "metadata": {
        "id": "2GfUl71qEoL8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to read Phone-Numbers from the text extracted from PDF documents"
      ],
      "metadata": {
        "id": "Q_8vcuaUJ3yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
        "\n",
        "def extract_phone_number(resume_text):\n",
        "    phone = re.findall(PHONE_REG, resume_text)\n",
        "\n",
        "    if phone:\n",
        "        number = ''.join(phone[0])\n",
        "\n",
        "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
        "            return number\n",
        "    return None"
      ],
      "metadata": {
        "id": "CgovQ2cSKUYQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to read Email from the text extracted from PDF documents"
      ],
      "metadata": {
        "id": "igMeU0RqLIoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
        "\n",
        "def extract_emails(resume_text):\n",
        "    return re.findall(EMAIL_REG, resume_text)"
      ],
      "metadata": {
        "id": "Es9u37p9LCI_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Function to print name, Phone Number, email from resume**"
      ],
      "metadata": {
        "id": "ePCvs9zeImxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    text = extract_text_from_pdf('/content/gdrive/MyDrive/Resume_Screening/Ankit_Gupta_Resume.pdf')\n",
        "    #print(text)\n",
        "    names = extract_names(text)\n",
        "    phone_number = extract_phone_number(text)\n",
        "    emails = extract_emails(text)\n",
        "    if names:\n",
        "        print(\"Name:\",names[0])\n",
        "    if phone_number:\n",
        "        print(\"Phone Number:\",phone_number)\n",
        "    if emails:\n",
        "        print(\"Email Address:\",emails[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMXfHW8YIjLP",
        "outputId": "e9666a23-4391-41c6-ae3b-0958eb342486"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Gupta Indian Institute\n",
            "Phone Number: 8292858632\n",
            "Email Address: ankitgupta8292@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extract Skills**"
      ],
      "metadata": {
        "id": "BqUNNOisMKOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define function to get skills and put them in technical/ non-technical based on csv provided "
      ],
      "metadata": {
        "id": "D2oiLVNrM0QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the csv files and prepare Technical & Non-Technical DB\n",
        "\n",
        "Technical_Skills_DB = []\n",
        "NONTechnical_Skills_DB = []\n",
        "\n",
        "df_nt = pd.read_csv('/content/gdrive/MyDrive/Resume_Screening/nontechnicalskills.csv',header=None)\n",
        "df_t = pd.read_csv('/content/gdrive/MyDrive/Resume_Screening/techskill.csv',header=None)"
      ],
      "metadata": {
        "id": "MBuCf9XTRyjE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Technical_Skills_DB = nltk.flatten(df_t.values.tolist())\n",
        "\n",
        "NONTechnical_Skills_DB = nltk.flatten(df_nt.values.tolist())\n",
        "print (NONTechnical_Skills_DB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6SOxKFbSFlT",
        "outputId": "ce013a51-0273-4b8a-d04f-1b441ba8731c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['coldcalling', 'teamwork', 'harwork', 'creativity', 'Collaboration', 'communication', 'accounting', 'sales', 'marketing', 'dataentry', 'Leadership skills', 'Adaptability and flexibility', 'Problem-solving', 'Decision-making', 'Creativity', 'Team-working', 'TimeManagement', 'Willingness-to-learn', nan, nan, nan, nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###  Function to extract Technical skills \n",
        "def extract_technical_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        "\n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        "\n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        "\n",
        "    # we create a set to keep the results in.\n",
        "    found_skills_technical = set()\n",
        "    \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in Technical_Skills_DB:\n",
        "            found_skills_technical.add(token)\n",
        "    \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in Technical_Skills_DB:\n",
        "            found_skills_technical.add(ngram)\n",
        "    \n",
        "    return found_skills_technical"
      ],
      "metadata": {
        "id": "8aXM2pLvWHHV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###  Function to extract Non-Technical skills \n",
        "def extract_nonTechnical_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        "\n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        "\n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        "\n",
        "    # we create a set to keep the results in.\n",
        "    found_skills_nontechnical = set()\n",
        "\n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "      if token.lower() in NONTechnical_Skills_DB:\n",
        "        found_skills_nontechnical.add(token)\n",
        "\n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in NONTechnical_Skills_DB:\n",
        "            found_skills_nontechnical.add(ngram)\n",
        "\n",
        "    return found_skills_nontechnical"
      ],
      "metadata": {
        "id": "ulBysLUxWY7U"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Function to print Technical & Non-Technical Skills from resume, and csv data provided**"
      ],
      "metadata": {
        "id": "oInHCP-UXA1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    technical_skills = extract_technical_skills(text)\n",
        "    print(\"Technical_skills:\", technical_skills )\n",
        "    nontechnical_skills = extract_nonTechnical_skills(text)\n",
        "    print(\"NON-Technical_skills:\", nontechnical_skills )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuLlPhfIMUGZ",
        "outputId": "6c5cf65c-7edc-4782-ee60-fda15cb1a387"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Technical_skills: {'Php', 'Java', 'MySQL', 'Github', 'CSS', 'newspaper', 'Framework', 'framework', 'C', 'github', 'Visual', 'PostgreSQL'}\n",
            "NON-Technical_skills: set()\n"
          ]
        }
      ]
    }
  ]
}